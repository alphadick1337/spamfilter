{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Uppgift 03\n",
        "Spamdetektering med engelska ord. Utökad tillämpning av textklassificering. Upphovspersoner = Colab - Chatgpt - Gemini - Axel"
      ],
      "metadata": {
        "id": "F0cxcgb7ltIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Här ligger datasetet, som du direkt laddar in i Colab utan omvägar: https://huggingface.co/datasets/ucirvine/sms_spam"
      ],
      "metadata": {
        "id": "WUBW-qggMzrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Du skall kort fylla i vissa uppgifer i ett Worddokument. Och ladda upp det med lösningen."
      ],
      "metadata": {
        "id": "glAU0piNKfyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steg 1 - Installera komponenter som behövs för Tensorflow. För publicering på nätet. Får du en varning om att starta om sessionen. Gör det, och och kör om från början!\n",
        "Fungerande Workaround för Tensorflow 2 (Webben)"
      ],
      "metadata": {
        "id": "02aA2YBcmvGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lösning: Flera textfiler + Grafer + Legacy Keras\n",
        "import os\n",
        "\n",
        "# 1. INSTALLERA TENSORFLOWJS\n",
        "# AXELS ANMÄRKNING. Efter första körningen kan du dektivera installen genom att\n",
        "# sätta ett # först på rad 7. Men GLÖM INTE DET SEN\n",
        "!pip install tensorflowjs\n",
        "\n",
        "# 2. TVINGA KERAS 2 (Legacy Mode - VIKTIGT FÖR ATT UNDVIKA FEL)\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "os.environ[\"TF_KERAS\"] = \"1\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflowjs as tfjs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import shutil\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from google.colab import files\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "bnkFzrjZ_5CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ladda dataset, och förbered data. Vi hämtar datasetet direkt via ett anrop till ucirvine/sms_spam\" se rad 9 i  koden. Inget behöver laddas upp i filer.\n",
        "Uppgift 1 - Ta ett skärmklipp på resultatet av cellen"
      ],
      "metadata": {
        "id": "iOPhaGEbobU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "\n",
        "ds = load_dataset(\"ucirvine/sms_spam\")\n",
        "texts = ds[\"train\"][\"sms\"]\n",
        "\n",
        "labels_raw = ds[\"train\"][\"label\"]\n",
        "def to01(y):\n",
        "    if isinstance(y, (int, np.integer)):\n",
        "        return int(y)\n",
        "    y = str(y).lower().strip()\n",
        "    return 1 if y == \"spam\" else 0\n",
        "\n",
        "labels = np.array([to01(y) for y in labels_raw], dtype=np.int32)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    np.array(texts, dtype=object),\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"Totalt antal poster: {len(texts)}\")\n",
        "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
        "print()\n",
        "print(\"Spam-andel i train:\", y_train.mean().round(3))"
      ],
      "metadata": {
        "id": "dJ5romf1pBgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "För att få en uppfattning om hur datan ser ut så listas de 10 första posterna i datasetet ut. Alltså av det som har hämtats.\n",
        "Spam = Spammeddelande Ham = Inte spam"
      ],
      "metadata": {
        "id": "aR31OGsLpx8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"De 10 första posterna i datasetet:\")\n",
        "for i in range(10):\n",
        "    print(f\"Text: {texts[i].strip()} - Label: {'Spam' if labels[i] == 1 else 'Ham'}\")"
      ],
      "metadata": {
        "id": "rivEKSWmp_UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer + padding (sparas för webben).Då krävs dessa filer för att modellen skall fungera. Det du skriver in på webbsidan, omvandlas till siffror, som sedan körs i modellen och testa. Och du får resultatet. Se längst ned för ett exempel."
      ],
      "metadata": {
        "id": "ZcNVhMDpq9Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "VOCAB_SIZE = 8000        # lagom för gymnasieprojekt\n",
        "MAX_LEN = 30             # SMS är korta\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN, lower=True)\n",
        "tokenizer.fit_on_texts(X_train.tolist())\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train.tolist())\n",
        "X_test_seq  = tokenizer.texts_to_sequences(X_test.tolist())\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "X_test_pad  = pad_sequences(X_test_seq,  maxlen=MAX_LEN, padding=\"post\", truncating=\"post\") # Osedd testdata\n",
        "\n",
        "oov_index = tokenizer.word_index.get(OOV_TOKEN, 1)\n",
        "\n",
        "# Spara tokenizer + metadata till webben\n",
        "with open(\"tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(tokenizer.to_json())\n",
        "\n",
        "# För enklare JS: spara word_index separat\n",
        "with open(\"word_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(tokenizer.word_index, f)\n",
        "\n",
        "with open(\"model_info.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\n",
        "        \"vocab_size\": VOCAB_SIZE,\n",
        "        \"max_len\": MAX_LEN,\n",
        "        \"oov_index\": int(oov_index),\n",
        "        \"labels\": [\"ham\", \"spam\"]\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"Exempel (text):\", X_train[0])\n",
        "print(\"Exempel (sekvens):\", X_train_seq[0][:10])\n",
        "print(\"OOV index:\", oov_index)\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRj1qrLNrTM8",
        "outputId": "3edb9bb9-eb49-43fa-97dc-55acc69dff5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exempel (text): Ok i am on the way to home hi hi\n",
            "\n",
            "Exempel (sekvens): [51, 2, 60, 19, 6, 132, 3, 78, 101, 101]\n",
            "OOV index: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uppgift för dig.\n",
        "Nu har ordförrådet byggts upp i tokenizern. Nu kan skriva in engelsk mening. Ord som är vanligt förekommande får ett lågt indexvärde, och sådana som inte finns med ges värdet 1 = oov = out of words. Jämför den första uppgiften du gjorde. Ju fler oov, desto sämre resultat av modellen."
      ],
      "metadata": {
        "id": "1IGlc-jssPMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uppgift 2 - Skriv en mening på engelska och ta ett skärmklipp av utfallet med tokens på den engelska respektive svenska meningen. Din slutsats?"
      ],
      "metadata": {
        "id": "c8R-x0p7K-4v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8230ff2d"
      },
      "source": [
        "Skriv in en engelsk mening nedan för att se hur den tokeniseras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8c0777b"
      },
      "source": [
        "english_sentence = input(\"Ange en engelsk mening: \")\n",
        "english_sequence = tokenizer.texts_to_sequences([english_sentence])\n",
        "print()\n",
        "print(f\"Din engelska mening: {english_sentence}\")\n",
        "print(f\"Tokeniserad sekvens (engelska): {english_sequence[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "870b8b31"
      },
      "source": [
        "Skriv nu in en svensk mening för att se hur den tokeniseras av samma tokenizer (observera att denna tokenizer är tränad på engelska data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f8d4e5c"
      },
      "source": [
        "swedish_sentence = input(\"Ange en svensk mening: \")\n",
        "swedish_sequence = tokenizer.texts_to_sequences([swedish_sentence])\n",
        "print()\n",
        "print(f\"Din svenska mening: {swedish_sentence}\")\n",
        "print(f\"Tokeniserad sekvens (svenska): {swedish_sequence[0]}\")\n",
        "print(\"Var det med många 1.or = Ordet finns inte med\")\n",
        "print()\n",
        "print(\"Modellen fungerar inte så bra (inte alls) på svenska ord och meningar...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nu är all data förbered så att den kan användas för träning i ett neural träningsmodell. Se separat info om den. Men lite kort! Se kommentarer i"
      ],
      "metadata": {
        "id": "0SpJclCjva-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32),\n",
        "    keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=64),\n",
        "    keras.layers.GlobalAveragePooling1D(),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "cb = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True)\n",
        "    # Här stoppas träningen med automatik. När den inte ger något mer. Inte alltid sant. Skydd mot överträning\n",
        "    # Om du kollar på rad 27. Så ser du max antalet epoker.\n",
        "    # Om du kollar på rad 29 så ser du batch-size. Antal data som matas in åt gången\n",
        "\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    callbacks=cb,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(\"Test accuracy:\", round(float(test_acc), 4))\n"
      ],
      "metadata": {
        "id": "EwTZJvVZvyBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visa träningen visuellet\n",
        "Uppgift 3 - Ta ett skärmklipp på träningen. Din åsikt?"
      ],
      "metadata": {
        "id": "QMuyN8pY15Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 1) # 1 row, 2 columns, first plot\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2) # 1 row, 2 columns, second plot\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QnPpOzEr2B0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fundera på. Är träningen bra?"
      ],
      "metadata": {
        "id": "hndl6f422ZBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visa i textbaserad Confusion Matrix.\n",
        "Uppgift 4. Ta ett skärmklipp. Verkar modellen vara bra?"
      ],
      "metadata": {
        "id": "27gABfPN2dGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob = model.predict(X_test_pad, verbose=0).reshape(-1)\n",
        "y_pred = (y_prob >= 0.5).astype(np.int32)\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=[\"ham\",\"spam\"]))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "h_AGVrxx2pBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ett confusion chart i diagram"
      ],
      "metadata": {
        "id": "30_u2UiM28yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y_zcuORu3FCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Förbered modellen för publicering på webben"
      ],
      "metadata": {
        "id": "A5KYVOCo5Afg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf tfjs_model\n",
        "model.save(\"spam_model.keras\")\n",
        "\n",
        "!tensorflowjs_converter \\\n",
        "  --input_format=keras \\\n",
        "  --output_format=tfjs_layers_model \\\n",
        "  spam_model.keras tfjs_model\n",
        "\n",
        "!ls -la tfjs_model\n"
      ],
      "metadata": {
        "id": "pWL9lCOG5QPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spara filerna i rätt format\n",
        "output_dir = 'tfjs_model_multi_files'\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "tfjs.converters.save_keras_model(model, output_dir)\n",
        "\n",
        "# Define variables needed for metadata\n",
        "word_index = tokenizer.word_index\n",
        "max_length = MAX_LEN\n",
        "padding_type = \"post\"\n",
        "trunc_type = \"post\"\n",
        "label_map = [\"ham\", \"spam\"] # Based on the definition in model_info.json\n",
        "oov_tok = OOV_TOKEN\n",
        "\n",
        "metadata = {\n",
        "    'word_index': word_index,\n",
        "    'max_length': max_length,\n",
        "    'padding_type': padding_type,\n",
        "    'trunc_type': trunc_type,\n",
        "    'labels': label_map,\n",
        "    'oov_token': oov_tok\n",
        "}\n",
        "with open(os.path.join(output_dir, 'metadata.json'), 'w', encoding='utf-8') as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "zip_filename = 'tfjs_multi_files.zip'\n",
        "!zip -r {zip_filename} {output_dir}\n",
        "\n",
        "print(f\"\\n✅ KLAR. Laddar ner '{zip_filename}'.\")\n",
        "files.download(zip_filename)\n",
        "print()"
      ],
      "metadata": {
        "id": "2JADKskjK6SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifiering av filerna. Kommer de att fungera på Github med hjälp av\n",
        "TFJS: 4.22.0. Detta är ett vanligt problem med webbpublicering. Se svaret längst ned!\n",
        "Uppgift 5. Ta ett skärmklipp av resultatet"
      ],
      "metadata": {
        "id": "SXoEqpvY6Yb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflowjs as tfjs # Required for loading the converted model\n",
        "import shutil # For cleaning up temporary directory\n",
        "\n",
        "# 1. Define paths and unzip the files\n",
        "zip_filename = 'tfjs_multi_files.zip'\n",
        "output_dir_verification = 'tfjs_temp_for_verification'\n",
        "\n",
        "# Clean up previous verification directory if it exists\n",
        "if os.path.exists(output_dir_verification):\n",
        "    shutil.rmtree(output_dir_verification)\n",
        "os.makedirs(output_dir_verification, exist_ok=True)\n",
        "\n",
        "print(f\"Unzipping '{zip_filename}' to '{output_dir_verification}'...\")\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_dir_verification)\n",
        "\n",
        "# The actual TFJS model files are inside 'tfjs_model_multi_files' within the zip\n",
        "tfjs_model_sub_path = os.path.join(output_dir_verification, 'tfjs_model_multi_files')\n",
        "metadata_path = os.path.join(tfjs_model_sub_path, 'metadata.json')\n",
        "model_json_path = os.path.join(tfjs_model_sub_path, 'model.json')\n",
        "\n",
        "print(f\"Successfully unzipped files to: {tfjs_model_sub_path}\")\n",
        "print(f\"Contents of '{tfjs_model_sub_path}': {os.listdir(tfjs_model_sub_path)}\")\n",
        "\n",
        "# 2. Load metadata\n",
        "print(\"\\n--- Loading Metadata (tokenizer information) ---\")\n",
        "with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "word_index = metadata['word_index']\n",
        "max_length = metadata['max_length']\n",
        "oov_token = metadata['oov_token']\n",
        "labels = metadata['labels']\n",
        "oov_index = word_index.get(oov_token, 1) # Get the actual numerical index for OOV\n",
        "\n",
        "print(f\"Max Length: {max_length}\")\n",
        "print(f\"OOV Token: '{oov_token}' (Index: {oov_index})\")\n",
        "print(f\"Labels: {labels}\")\n",
        "\n",
        "# 3. Load the TFJS converted model back into Keras for verification\n",
        "print(\"\\n--- Attempting to load TFJS model (model.json and bin files) into Keras ---\")\n",
        "try:\n",
        "    loaded_tfjs_model = tfjs.converters.load_keras_model(model_json_path)\n",
        "    print(\"✅ Successfully loaded TFJS model into Keras!\")\n",
        "    loaded_tfjs_model.summary()\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading TFJS model: {e}\")\n",
        "    # If the model cannot be loaded, the verification process stops here.\n",
        "    # Clean up and exit\n",
        "    shutil.rmtree(output_dir_verification)\n",
        "    raise\n",
        "\n",
        "# 4. Create a preprocessing function mimicking client-side (JavaScript) logic\n",
        "def preprocess_text_for_web(text, word_index, max_length, oov_index):\n",
        "    # Tokenize: Split text into words, convert to lowercase\n",
        "    words = text.lower().split()\n",
        "    sequence = []\n",
        "    for word in words:\n",
        "        # Get index, use OOV index if not found\n",
        "        sequence.append(word_index.get(word, oov_index))\n",
        "\n",
        "    # Pad/Truncate sequence\n",
        "    if len(sequence) > max_length:\n",
        "        processed_sequence = sequence[:max_length] # Truncate from the end\n",
        "    else:\n",
        "        processed_sequence = sequence + [0] * (max_length - len(sequence)) # Pad with zeros\n",
        "\n",
        "    # Ensure it's a numpy array of integers for the model\n",
        "    return np.array([processed_sequence], dtype=np.int32)\n",
        "\n",
        "# 5. Make predictions using the loaded model and preprocessor\n",
        "def make_prediction_with_web_model(text, model, word_index, max_length, oov_index, labels):\n",
        "    processed_input = preprocess_text_for_web(text, word_index, max_length, oov_index)\n",
        "    prediction_prob = model.predict(processed_input, verbose=0)[0][0]\n",
        "    prediction_label_index = (prediction_prob >= 0.5).astype(int)\n",
        "    predicted_label = labels[prediction_label_index]\n",
        "    return prediction_prob, predicted_label\n",
        "\n",
        "print(\"\\n---  Testar förutsägeler med modellen ---\")\n",
        "test_sentences = [\n",
        "    \"Congratulations! You have won a free iPhone. Claim your prize now!\",  # Spam example\n",
        "    \"Hi, how are you doing today? I wanted to check in about our meeting.\",               # Ham example\n",
        "    \"URGENT! Your account needs verification. Click here to avoid suspension.\", # Another spam\n",
        "    \"Could you please send me the report by end of day? Thanks!\"             # Another ham\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    prob, label = make_prediction_with_web_model(sentence, loaded_tfjs_model, word_index, max_length, oov_index, labels)\n",
        "    print(f\"Text: '{sentence}'\")\n",
        "    print(f\"  Prediction: {label} (Probability: {prob:.4f})\")\n",
        "\n",
        "# 6. Clean up temporary files\n",
        "print(\"\\n--- Cleaning up temporary files ---\")\n",
        "shutil.rmtree(output_dir_verification)\n",
        "print(f\"Cleaned up temporary directory: {output_dir_verification}\")\n",
        "print()\n",
        "print(\"\\n✅ Verification complete. Testerna säger att de skapade filerna skall fungera. Vi får väl se\")\n",
        "print()\n",
        "print(\"Ladda ned zipfilen från files /content/tfjs_multi_files.zip\")"
      ],
      "metadata": {
        "id": "MNu8ISAz63SQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}